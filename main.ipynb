{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlaNgLVPjxx"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99-LnExbP-Qc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "import torch.distributed as dist # Maybe in the future we need multiple gpu's for training for now this is just a starting point\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "from typing import List\n",
        "import random\n",
        "import os\n",
        "import datetime\n",
        "import subprocess\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "\n",
        "from PIL import Image, ImageFilter, ImageOps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "PcA7dQqLXi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    \"\"\"\n",
        "    Apply Gaussian Blur to the PIL image.\n",
        "    \"\"\"\n",
        "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n",
        "        self.prob = p\n",
        "        self.radius_min = radius_min\n",
        "        self.radius_max = radius_max\n",
        "\n",
        "    def __call__(self, img):\n",
        "        do_it = random.random() <= self.prob\n",
        "        if not do_it:\n",
        "            return img\n",
        "\n",
        "        return img.filter(\n",
        "            ImageFilter.GaussianBlur(\n",
        "                radius=random.uniform(self.radius_min, self.radius_max)\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "class Solarization(object):\n",
        "    \"\"\"\n",
        "    Apply Solarisation to the PIL image.\n",
        "    \"\"\"\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p:\n",
        "            return ImageOps.solarize(img)\n",
        "        else:\n",
        "            return img"
      ],
      "metadata": {
        "id": "xOXQn0LcXBTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRYOxRLH4_qo"
      },
      "source": [
        "### Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Ph-vPi4_qp",
        "outputId": "98de04da-6f89-4844-e19e-567b9402f87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ]
        }
      ],
      "source": [
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness.\n",
        "  NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyW1VV86eMxn"
      },
      "source": [
        "more or less the same as original one i didn't changed much just added couple of things here and there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGU1nFicPn36"
      },
      "source": [
        "### Arguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/facebookresearch/dino/blob/main/main_dino.py get_args_parser() for explanations\n",
        "\n",
        "class args_class:\n",
        "  def __init__(self):\n",
        "    # Model Parameters\n",
        "    self.MOMENTUM_TEACHER = 0.996     #default 0.996\n",
        "    self.OUT_DIM = 65536 #default 65536\n",
        "    self.NORM_LAST_LAYER = True\n",
        "    self.USE_BN_IN_HEAD = False     #default False\n",
        "\n",
        "    # DINO LOSS Paramters\n",
        "    self.CENTER_MOMENTUM = 0.9 # default 0.9\n",
        "\n",
        "    # Temperature Student Parameters\n",
        "    self.STUDENT_TEMP = 0.1 # default 0.1\n",
        "\n",
        "    # Temperature Teacher Parameters\n",
        "    self.WARMUP_TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.WARMUP_TEACHER_TEMP_EPOCHS = 30 #default 30, but erroneously default 0 in the DINO paper?\n",
        "\n",
        "    # Training / Optimizations Parameters\n",
        "    self.USE_FP16 = True #default True\n",
        "    self.WEIGHT_DECAY = 0.04 #default 0.04\n",
        "    self.WEIGHT_DECAY_END = 0.4 # default 0.4\n",
        "    self.CLIP_GRAD = 3.0 #default 3.0\n",
        "    self.BATCH_SIZE_PER_GPU = 64 #default 64\n",
        "    self.EPOCHS = 100 #default 100\n",
        "    self.FREEZE_LAST_LAYER = 1 #default 1\n",
        "    self.LR = 0.0005 #default 0.0005\n",
        "    self.WARMUP_EPOCHS = 10 #default 10\n",
        "    self.MIN_LR = 1e-6 #default 1e-6\n",
        "    self.optimizer = 'adamw' # default adamw    TODO:Could be constructed here? not sure.\n",
        "    self.DROP_PATH_RATE = 0.1 #default 0.1\n",
        "\n",
        "    # Multi-Crop Parameters\n",
        "    self.GLOBAL_CROPS_NUMBER = 2 #default 2\n",
        "    self.GLOBAL_CROPS_SCALE = (0.4, 0.1) # default (0.4, 0.1)\n",
        "    self.LOCAL_CROPS_NUMBER = 8 #default 8\n",
        "    self.LOCAL_CROPS_SCALE = (0.05, 0.4) #default (0.05, 0.4)\n",
        "\n",
        "    #Misc\n",
        "    self.num_works = 10 # default 10\n",
        "    # TODO: imgnet directory, how to store weights?\n",
        "\n",
        "args = args_class()"
      ],
      "metadata": {
        "id": "h0jKeWbKP_kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VByRjDOMQejt"
      },
      "source": [
        "### Architecture, DINOHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBZl_GfjQgFK"
      },
      "outputs": [],
      "source": [
        "class DINOHead(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
        "    super().__init__()\n",
        "    nlayers = max(nlayers, 1)\n",
        "    if nlayers == 1:\n",
        "      self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
        "    else:\n",
        "      layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "      if use_bn:\n",
        "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "      layers.append(nn.GELU())\n",
        "      for _ in range(nlayers - 2):\n",
        "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        if use_bn:\n",
        "          layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "      layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "      self.mlp = nn.Sequential(*layers)\n",
        "    self.apply(self._init_weights)\n",
        "    self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
        "    self.last_layer.weight_g.data.fill_(1)\n",
        "    if norm_last_layer:\n",
        "      self.last_layer.weight_g.requires_grad = False\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "        trunc_normal_(m.weight, std=.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.mlp(x)\n",
        "      x = nn.functional.normalize(x, dim=-1, p=2)\n",
        "      x = self.last_layer(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ7d1em4_qz"
      },
      "source": [
        "### ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z8zAYxU4_q0",
        "outputId": "73c9a645-ff60-4731-97cd-f6c085ad8a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weights are close? : False\n",
            "If not, dont expect same logits\n",
            "weights are close? : True\n",
            "If not, dont expect same logits\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_id: 0, ourlogits: -0.09006007760763168, reallogits: -0.09006007760763168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Our copy of BasicBlock from torchvision.models.resnet\n",
        "\n",
        "    \"\"\"\n",
        "    # To increease the final number of output channels\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels:int,\n",
        "        stride:int = 1,\n",
        "        downsample = None,\n",
        "        groups:int = 1,\n",
        "        base_width:int = 64,\n",
        "        dilation:int = 1,\n",
        "        norm_layer = None\n",
        "\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        # Groups are != 1 when we use different filters for different channels of the image\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "\n",
        "        # Dilation means adding \"empty\" pixels in the kernel, effectively making it bigger\n",
        "        # Dilation == 1 means no dilation\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "\n",
        "        # Note: names bn1, bn2 etc are kept even if norm_layer is not BatchNorm2d\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride) # conv3x3 is standard 3x3 conv\n",
        "        self.bn1 = norm_layer(out_channels) # Normalise each feature map across spatial and batch dimension\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = norm_layer(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x  # Store original input\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleneckConvBlock(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    For resnet50 and above, insted of simply residual blocks, they used bottleneck blocks.\n",
        "\n",
        "    They say it was more for computational efficiency than accuracy.\n",
        "\n",
        "\n",
        "    In the original paper https://arxiv.org/abs/1512.03385 they used stride 1x1\n",
        "    in the first layer of the bottleneck block.\n",
        "\n",
        "    Apparently 3x3 is better and is what is implemented in torchvision.models.resnet\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    expansion: int = 4 # channels in the output is 4 times the number of channels in the input\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample = None,\n",
        "        groups: int = 1, # Bottleneck blocks can have parallel convolutions with groups!=1 ( ResNeXt)\n",
        "        base_width: int = 64, # How many channels in the internal 3x3 conv of each group\n",
        "        dilation: int = 1,\n",
        "        norm_layer = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        width = int(inplanes * (base_width / 64.0) ) * groups #(64 was the default val for base_width)\n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, width) # 1x1 conv to reduce the number of channels\n",
        "        self.bn1 = norm_layer(width)\n",
        "\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # the actual convolution in the block\n",
        "        self.bn2 = norm_layer(width)\n",
        "\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion) # 1x1 conv to increase the number of channels\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample # Downsample is used to match the number of channels in the input and output\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class OurResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Our copy of ResNet from torchvision.models.resnet\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block, # block can be either ResidualConvBlock or Bottleneck class\n",
        "        layers: List[int], # List of residual blocks in each layer, e.g. [2, 2, 2, 2] for ResNet18\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False, # not implemented Zero-initialize the last BN in each residual branch,\n",
        "        groups: int = 1, # in Bottleneck\n",
        "        width_per_group: int = 64, # in Bottleneck\n",
        "        replace_stride_with_dilation: List[bool] = None, # in Bottleneck\n",
        "        norm_layer = None\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "\n",
        "        # Resnet has 4 layer gropus, only in the last 3 dilation can substitute stride\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # Reference for the following is Table 1 of ResNet paper https://arxiv.org/abs/1512.03385\n",
        "\n",
        "        # First conv is 7x7 in avery resnet, stride 2, padding 3\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Maxpool is 3x3 with stride 2 and padding 1\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Initialize the layer groups\n",
        "        # Numbers 64,128,256,512 get multiplyied by the expansion factor of the block (1 for ResidualConvBlock, 4 for BottleneckConvBlock)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # I just copied this zero_init_residual part\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BottleneckConvBlock) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BottleneckConvBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    # With this, we can create any of the layer groups in square brackets(x2,x3,x6,x36 etc)\n",
        "    # of Table 1 of the ResNet paper\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block, # kind of block, can be either ResidualConvBlock or BottleneckConvBlock class\n",
        "        planes: int,\n",
        "        blocks: int, # number of blocks in the layer group\n",
        "        stride: int = 1,\n",
        "        dilate: bool = False, # if True, use dilation instead of stride\n",
        "    ) -> nn.Sequential:\n",
        "\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer))\n",
        "\n",
        "        # We always have minimum 2 blocks in each layer group\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "def _resnet(\n",
        "    block,\n",
        "    layers: List[int],\n",
        "    weights,\n",
        "    progress: bool,\n",
        "    **kwargs,\n",
        ") -> OurResNet:\n",
        "\n",
        "    if weights is not None:\n",
        "        # _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "        raise NotImplementedError(\"Weights upload not implemented yet\")\n",
        "\n",
        "    model = OurResNet(block, layers, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def ourResnet18( progress: bool = True, **kwargs ) -> OurResNet:\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    return _resnet(\n",
        "        ResidualConvBlock,\n",
        "        [2, 2, 2, 2],\n",
        "        weights=None,  # No weights for now\n",
        "        progress=progress,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "import torchvision\n",
        "\n",
        "ourRes18 = ourResnet18(num_classes=10).to(DEVICE)\n",
        "realRes18 = torchvision.models.resnet18(num_classes=10,weights=None).to(DEVICE)\n",
        "\n",
        "print(f\"weights are close? : {torch.allclose(ourRes18.conv1.weight, realRes18.conv1.weight)}\")\n",
        "print(f\"If not, dont expect same logits\")\n",
        "\n",
        "# To have the same weights, they need to have the same exact initialization\n",
        "ourRes18.load_state_dict(realRes18.state_dict())\n",
        "\n",
        "print(f\"weights are close? : {torch.allclose(ourRes18.conv1.weight, realRes18.conv1.weight)}\")\n",
        "print(f\"If not, dont expect same logits\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqa9k8m44_q4"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dino_schedules(args, n_iterations_per_epoch):\n",
        "    \"\"\"\n",
        "    Creates the learning rate, weight decay, and teacher momentum schedules\n",
        "    as pre-calculated NumPy arrays, matching the original DINO implementation.\n",
        "\n",
        "    Args:\n",
        "        args (args_class): An object containing training hyperparameters.\n",
        "        n_iterations_per_epoch (int): The number of training steps in one epoch.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lr_schedule (np.array): The learning rate for each iteration.\n",
        "            - wd_schedule (np.array): The weight decay for each iteration.\n",
        "            - momentum_schedule (np.array): The teacher momentum for each iteration.\n",
        "    \"\"\"\n",
        "    # Total number of training iterations\n",
        "    n_iterations = args.EPOCHS * n_iterations_per_epoch\n",
        "\n",
        "    # Helper function to replicate the original DINO cosine scheduler\n",
        "    def cosine_scheduler(base_value, final_value, total_iters, warmup_iters=0, start_warmup_value=0):\n",
        "        warmup_schedule = np.array([])\n",
        "        if warmup_iters > 0:\n",
        "            warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
        "\n",
        "        iters = np.arange(total_iters - warmup_iters)\n",
        "        schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n",
        "\n",
        "        schedule = np.concatenate((warmup_schedule, schedule))\n",
        "        assert len(schedule) == total_iters\n",
        "        return schedule\n",
        "\n",
        "    # Linear scaling rule from the original paper: base_lr * batch_size / 256\n",
        "    # Our code is single-GPU, so we use BATCH_SIZE_PER_GPU. For multi-GPU, this would be scaled by world_size.\n",
        "    base_lr = args.LR * args.BATCH_SIZE_PER_GPU / 256.0\n",
        "\n",
        "    lr_schedule = cosine_scheduler(\n",
        "        base_value=base_lr,\n",
        "        final_value=args.MIN_LR,\n",
        "        total_iters=n_iterations,\n",
        "        warmup_iters=args.WARMUP_EPOCHS * n_iterations_per_epoch,\n",
        "        start_warmup_value=0, # Start warmup from 0\n",
        "    )\n",
        "\n",
        "    # No warmup for weight decay\n",
        "    wd_schedule = cosine_scheduler(\n",
        "        base_value=args.WEIGHT_DECAY,\n",
        "        final_value=args.WEIGHT_DECAY_END,\n",
        "        total_iters=n_iterations,\n",
        "    )\n",
        "\n",
        "    # No warmup for momentum\n",
        "    momentum_schedule = cosine_scheduler(\n",
        "        base_value=args.MOMENTUM_TEACHER,\n",
        "        final_value=1.0,\n",
        "        total_iters=n_iterations,\n",
        "    )\n",
        "\n",
        "    return lr_schedule, wd_schedule, momentum_scheduleclass DINOHead(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
        "    super().__init__()\n",
        "    nlayers = max(nlayers, 1)\n",
        "    if nlayers == 1:\n",
        "      self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
        "    else:\n",
        "      layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "      if use_bn:\n",
        "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "      layers.append(nn.GELU())\n",
        "      for _ in range(nlayers - 2):\n",
        "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        if use_bn:\n",
        "          layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "      layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "      self.mlp = nn.Sequential(*layers)\n",
        "    self.apply(self._init_weights)\n",
        "    self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
        "    self.last_layer.weight_g.data.fill_(1)\n",
        "    if norm_last_layer:\n",
        "      self.last_layer.weight_g.requires_grad = False\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "        trunc_normal_(m.weight, std=.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.mlp(x)\n",
        "      x = nn.functional.normalize(x, dim=-1, p=2)\n",
        "      x = self.last_layer(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "azsDsBe-zl_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2EFhlzPPxEq"
      },
      "source": [
        "### Loss, Train one Epoch, Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QDNt2MrVtmq"
      },
      "outputs": [],
      "source": [
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The loss function encourages a student network to match the output of a momentum teacher network.\n",
        "    This is a form of self-distillation without labels.\n",
        "\n",
        "    Args:\n",
        "        args (args_class): An object containing the necessary hyperparameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # The student temperature is constant\n",
        "        self.student_temp = args.STUDENT_TEMP\n",
        "        # The number of crops is the two global crops plus all local crops\n",
        "        self.n_crops = args.GLOBAL_CROPS_NUMBER + args.LOCAL_CROPS_NUMBER\n",
        "        self.center_momentum = args.CENTER_MOMENTUM\n",
        "        self.register_buffer(\"center\", torch.zeros(1, args.OUT_DIM)) # Initialize a non-trainable tensor and add to the module\n",
        "\n",
        "        # The teacher temperature is scheduled to warm up from an initial value to a final value.\n",
        "        # The DINO paper mentions a warmup from 0.04 to 0.07 over 30 epochs.\n",
        "        teacher_schedule  = torch.linspace(\n",
        "            args.WARMUP_TEACHER_TEMP,\n",
        "            args.TEACHER_TEMP,\n",
        "            args.EPOCHS,\n",
        "        )\n",
        "        self.register_buffer(\"teacher_temp_schedule\", teacher_schedule)\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Calculates the DINO loss.\n",
        "\n",
        "        The student is trained to match the teacher's output. The teacher's output is centered\n",
        "        and sharpened. The loss is computed as the cross-entropy between the teacher's and\n",
        "        student's probability distributions over different views.\n",
        "\n",
        "        Args:\n",
        "            student_output (torch.Tensor): The output of the student network for all crops.\n",
        "            teacher_output (torch.Tensor): The output of the teacher network for global crops.\n",
        "            epoch (int): The current training epoch, used for the temperature schedule.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The calculated DINO loss.\n",
        "        \"\"\"\n",
        "        # student side\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = student_out.chunk(self.n_crops)\n",
        "\n",
        "        # teacher side: center & sharpen\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach().chunk(2)\n",
        "\n",
        "        total_loss, n_loss_terms = 0.0, 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v, sv in enumerate(student_out):\n",
        "                if v == iq:\n",
        "                    continue\n",
        "                loss = torch.sum(-q * F.log_softmax(sv, dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "\n",
        "        total_loss /= n_loss_terms\n",
        "        self._update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update the center used for teacher output centering.\n",
        "        This is a momentum update using the global batch statistics.\n",
        "\n",
        "        This operation is performed in-place and without gradients.\n",
        "        \"\"\"\n",
        "        # In DDP, teacher_output is the output for the local batch on the current process.\n",
        "        # We need the mean of the outputs across all processes.\n",
        "        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)\n",
        "\n",
        "        # synchronize across all processes\n",
        "        if dist.is_initialized():\n",
        "            dist.all_reduce(batch_center, op=dist.ReduceOp.SUM)\n",
        "            batch_center /= dist.get_world_size()\n",
        "\n",
        "        # momentum update\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGIjTGSiVtqC"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(student, teacher, dino_loss, data_loader,\n",
        "                    optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
        "                    epoch, loss_scaler, args):\n",
        "    \"\"\"\n",
        "    Trains the DINO model for one epoch using bfloat16 mixed-precision.\n",
        "\n",
        "    Args:\n",
        "        student (nn.Module): The student network.\n",
        "        teacher (nn.Module): The teacher network.\n",
        "        dino_loss (nn.Module): The DINO loss function.\n",
        "        data_loader (torch.utils.data.DataLoader): The data loader for the training set.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer for the student network.\n",
        "        lr_schedule (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler.\n",
        "        wd_schedule (torch.optim.lr_scheduler._LRScheduler): The weight decay scheduler.\n",
        "        momentum_schedule (torch.optim.lr_scheduler._LRScheduler): The momentum scheduler for the teacher.\n",
        "        epoch (int): The current epoch number.\n",
        "        loss_scaler (torch.cuda.amp.GradScaler): The gradient scaler for mixed-precision training.\n",
        "        args (args_class): The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss for the epoch.\n",
        "    \"\"\"\n",
        "    pbar = tqdm(data_loader)\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for it, (images, _) in enumerate(pbar):\n",
        "        # Update learning rate and weight decay per iteration\n",
        "        for i, param_group in enumerate(optimizer.param_groups):\n",
        "            param_group[\"lr\"] = lr_schedule[it]\n",
        "            if i == 0:  # only the first group is regularized\n",
        "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
        "\n",
        "        # Move images to the GPU\n",
        "        images = [im.cuda(non_blocking=True) for im in images]\n",
        "\n",
        "        # Forward pass with bfloat16 automatic mixed-precision\n",
        "        # Note: bfloat16 requires NVIDIA Ampere (e.g. A100) or newer GPUs.\n",
        "        with torch.cuda.amp.autocast(enabled=args.USE_FP16, dtype=torch.bfloat16):\n",
        "            teacher_output = teacher(images[:2]) # Global crops only\n",
        "            student_output = student(images)\n",
        "            loss = dino_loss(student_output, teacher_output, epoch)\n",
        "\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"Loss is {loss}, stopping training\")\n",
        "            # Save model weights before exiting\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'student_state_dict': student.state_dict(),\n",
        "                'teacher_state_dict': teacher.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "            }, 'model_checkpoint_infinite_loss.pth')\n",
        "            print(\"Model weights saved to model_checkpoint_infinite_loss.pth\")\n",
        "            import sys\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Backward pass and student update\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # The GradScaler scales the loss to prevent underflow\n",
        "        loss_scaler.scale(loss).backward()\n",
        "        if args.CLIP_GRAD > 0:\n",
        "            # Unscale gradients before clipping\n",
        "            loss_scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(student.parameters(), args.CLIP_GRAD)\n",
        "        loss_scaler.step(optimizer)\n",
        "        loss_scaler.update()\n",
        "\n",
        "        # EMA update for the teacher network\n",
        "        with torch.no_grad():\n",
        "            m = momentum_schedule[it]\n",
        "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
        "\n",
        "        # Logging\n",
        "        torch.cuda.synchronize()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_description(f\"Epoch {epoch} | Loss: {loss.item():.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "    return train_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0xBB6arP_6s"
      },
      "outputs": [],
      "source": [
        "class CustomGaussianBlur(object):\n",
        "    def __init__(self, strength=1.0):\n",
        "        self.blur = GaussianBlur(p=1.0 if strength > 0.5 else 0.1)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.blur(img)\n",
        "\n",
        "\n",
        "class CustomSolarisation(object):\n",
        "    def __init__(self, p=0.2):\n",
        "        self.solar = Solarization(p=p)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.solar(img)\n",
        "\n",
        "\n",
        "class DataAugmentationDINO(object):\n",
        "    def __init__(self, global_crops_scale, local_crops_scale, local_crops_number):\n",
        "\n",
        "        flip_and_color_jitter = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply(\n",
        "                [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
        "                p=0.8\n",
        "            ),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "        ])\n",
        "\n",
        "        normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "        # Global crop 1: strong blur, no solarisation\n",
        "        self.global_transfo1 = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            CustomGaussianBlur(strength=1.0),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        # Global crop 2: weak blur, solarisation\n",
        "        self.global_transfo2 = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            CustomGaussianBlur(strength=0.1),\n",
        "            CustomSolarisation(p=0.2),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        # Local crop transformation: moderate blur\n",
        "        self.local_crops_number = local_crops_number\n",
        "        self.local_transfo = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC),\n",
        "            flip_and_color_jitter,\n",
        "            CustomGaussianBlur(strength=0.5),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    def __call__(self, image):\n",
        "        crops = []\n",
        "        crops.append(self.global_transfo1(image))\n",
        "        crops.append(self.global_transfo2(image))\n",
        "        for _ in range(self.local_crops_number):\n",
        "            crops.append(self.local_transfo(image))\n",
        "        return crops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4kMu9WbBo-C"
      },
      "source": [
        "##MultiCropWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DLNYJI_B130"
      },
      "outputs": [],
      "source": [
        "class MultiCropWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    This is essential in DINO-style self-supervised learning, where each image\n",
        "    is transformed into several \"views\" (global and local crops) and all\n",
        "    views must be processed efficiently.\n",
        "\n",
        "    Args:\n",
        "        backbone (nn.Module):  Feature extractor ( ResNet --> from pietro).\n",
        "        head (nn.Module):      head (MLP) mapping extracted features to the output space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone: nn.Module = None, head:nn.Module=None):# : hint for the expected type\n",
        "        super().__init__()\n",
        "        # Store the backbone and head modules\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, crops: List[Tensor]) -> List[Tensor]: #list of image crops (each crop is a tensor)\n",
        "        \"\"\"\n",
        "        Performs a single forward pass for multiple crops of each image.\n",
        "\n",
        "        -- Concatenate all crop tensors along the batch dimension.\n",
        "          num of crops per image multiple the batch size to get the total batch size.\n",
        "\n",
        "        -- Run the entire batch through the Mpdel to extract features.\n",
        "\n",
        "        -- Flatten spatial dimensions if needed, resulting in (N*B, D_flat).\n",
        "\n",
        "        -- Split the heading back into a list of N tensors, each of shape (B, out_dim).\n",
        "\n",
        "        Args:\n",
        "            crops (List[Tensor]):\n",
        "                A list of image batches.\n",
        "                The list length equals the number of crops per image.\n",
        "\n",
        "        Returns:\n",
        "            List[Tensor]:\n",
        "                A list of head outputs (headings), each of shape (B, out_dim),\n",
        "                in the same order as the input crops.\n",
        "        \"\"\"\n",
        "        # Concatenate all crops into one large batch\n",
        "        all_crops = torch.cat(crops, dim=0)\n",
        "\n",
        "        # Feature extraction through thr model\n",
        "        features = self.backbone(all_crops)\n",
        "\n",
        "        # Flatten--->if needed\n",
        "        if features.dim() > 2:\n",
        "            features = torch.flatten(features, start_dim=1)\n",
        "\n",
        "        # Project features to the dino head which is MLP\n",
        "        heading = self.head(features)\n",
        "\n",
        "\n",
        "        #  Split heading_inp back again\n",
        "        #  divides the first dim into `len(crops)` equal parts of size B\n",
        "        outputs = list(heading.chunk(len(crops), dim=0))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RZx6z6GePtUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aTeZt-DQAVo"
      },
      "outputs": [],
      "source": [
        "def train_dino(args):\n",
        "    print(\"I am a placeholder for the main training function.\")\n",
        "    # 1. init rand seed\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 2. prep data (build dataset, dataloader)\n",
        "    dataset = datasets.ImageFolder(args.data_path, transform=DataAugmentationDINO(\n",
        "            args.GLOBAL_CROPS_SCALE,\n",
        "            args.LOCAL_CROPS_SCALE,\n",
        "            args.LOCAL_CROPS_NUMBER,\n",
        "        ))\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.BATCH_SIZE_PER_GPU,\n",
        "        num_workers=args.num_works,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # 3. build student, teacher, loss, optimizer\n",
        "    # Backbone: ResNet50 as an example.\n",
        "    backbone = models.resnet50(pretrained=False)\n",
        "    backbone.fc = nn.Identity() # Replace the final FC layer with an identity layer\n",
        "    in_dim = backbone.fc.in_features\n",
        "\n",
        "    # Head for DINO\n",
        "    head = DinoHead(\n",
        "        in_dim,\n",
        "        args.OUT_DIM,\n",
        "        use_bn=args.USE_BN_IN_HEAD,\n",
        "        norm_last_layer=args.NORM_LAST_LAYER,\n",
        "    )\n",
        "\n",
        "    student = MultiCropWrapper(backbone, head)\n",
        "    teacher = MultiCropWrapper(models.resnet50(pretrained=False), DinoHead(\n",
        "        in_dim,\n",
        "        args.OUT_DIM,\n",
        "        use_bn=args.USE_BN_IN_HEAD,\n",
        "        norm_last_layer=args.NORM_LAST_LAYER,\n",
        "    ))\n",
        "\n",
        "    student = student.cuda()\n",
        "    teacher = teacher.cuda()\n",
        "\n",
        "    teacher.load_state_dict(student.state_dict())\n",
        "    for p in teacher.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    dino_loss = DINOLoss(args).cuda()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(student.parameters(), lr=args.LR, weight_decay=args.WEIGHT_DECAY)\n",
        "\n",
        "    # 4. init schedulers\n",
        "    #    This is where you would call the new function\n",
        "    n_iter_per_epoch = len(data_loader)\n",
        "    lr_schedule, wd_schedule, momentum_schedule = get_dino_schedules(args, n_iter_per_epoch)\n",
        "\n",
        "    # 5. init loss_scaler\n",
        "    loss_scaler = torch.cuda.amp.GradScaler(enabled=args.USE_FP16)\n",
        "\n",
        "    # 6. train loop\n",
        "    for epoch in range(args.EPOCHS):\n",
        "        # Train one epoch\n",
        "        train_stats = train_one_epoch(student, teacher, dino_loss, data_loader,\n",
        "                                      optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
        "                                      epoch, loss_scaler, args)\n",
        "\n",
        "        print(f\"Epoch {epoch} training stats: {train_stats}\")\n",
        "\n",
        "        # Save checkpoints\n",
        "        if epoch % 10 == 0 or epoch == args.EPOCHS - 1:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'student_state_dict': student.state_dict(),\n",
        "                'teacher_state_dict': teacher.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss_scaler_state_dict': loss_scaler.state_dict(),\n",
        "                'args': args,\n",
        "            }, f'{args.output_dir}/checkpoint_{epoch:04d}.pth')\n",
        "            print(f\"Checkpoint saved to {args.output_dir}/checkpoint_{epoch:04d}.pth\")\n",
        "\n",
        "    print(\"DINO training finished!\")\n",
        "\n"
      ]
    }
  ]
}