{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "thlaNgLVPjxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "import sys\n",
        "import time\n",
        "import torch.distributed as dist # Maybe in the future we need multiple gpu's for training for now this is just a starting point\n",
        "import math"
        "from torch import Tensor\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "99-LnExbP-Qc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "more or less the same as original one i didn't changed much just added couple of things here and there"
      ],
      "metadata": {
        "id": "VyW1VV86eMxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments"
      ],
      "metadata": {
        "id": "PGU1nFicPn36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/facebookresearch/dino/blob/main/main_dino.py get_args_parser() for explanations\n",
        "\n",
        "class args_class:\n",
        "  def __init__(self):\n",
        "    # Model Parameters\n",
        "    self.MOMENTUM_TEACHER = 0.996     #default 0.996\n",
        "    self.OUT_DIM = 65536 #default 65536\n",
        "    self.NORM_LAST_LAYER = True\n",
        "    self.USE_BN_IN_HEAD = False     #default False\n",
        "\n",
        "    # DINO LOSS Paramters\n",
        "    self.CENTER_MOMENTUM = 0.9 # default 0.9\n",
        "\n",
        "    # Temperature Student Parameters\n",
        "    self.STUDENT_TEMP = 0.1 # default 0.1\n",
        "\n",
        "    # Temperature Teacher Parameters\n",
        "    self.WARMUP_TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.WARMUP_TEACHER_TEMP_EPOCHS = 30 #default 30, but erroneously default 0 in the DINO paper?\n",
        "\n",
        "    # Training / Optimizations Parameters\n",
        "    self.USE_FP16 = True #default True\n",
        "    self.WEIGHT_DECAY = 0.04 #default 0.04\n",
        "    self.WEIGHT_DECAY_END = 0.4 # default 0.4\n",
        "    self.CLIP_GRAD = 3.0 #default 3.0\n",
        "    self.BATCH_SIZE_PER_GPU = 64 #default 64\n",
        "    self.EPOCHS = 100 #default 100\n",
        "    self.FREEZE_LAST_LAYER = 1 #default 1\n",
        "    self.LR = 0.0005 #default 0.0005\n",
        "    self.WARMUP_EPOCHS = 10 #default 10\n",
        "    self.MIN_LR = 1e-6 #default 1e-6\n",
        "    self.optimizer = 'adamw' # default adamw    TODO:Could be constructed here? not sure.\n",
        "    self.DROP_PATH_RATE = 0.1 #default 0.1\n",
        "\n",
        "    # Multi-Crop Parameters\n",
        "    self.GLOBAL_CROPS_NUMBER = 2 #default 2\n",
        "    self.GLOBAL_CROPS_SCALE = (0.4, 0.1) # default (0.4, 0.1)\n",
        "    self.LOCAL_CROPS_NUMBER = 8 #default 8\n",
        "    self.LOCAL_CROPS_SCALE = (0.05, 0.4) #default (0.05, 0.4)\n",
        "\n",
        "    #Misc\n",
        "    self.num_works = 10 # default 10\n",
        "    # TODO: imgnet directory, how to store weights?\n",
        "\n",
        "args = args_class()"
      ],
      "metadata": {
        "id": "h0jKeWbKP_kT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture, DINOHead"
      ],
      "metadata": {
        "id": "VByRjDOMQejt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBZl_GfjQgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "PcA7dQqLXi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dino_schedules(args, n_iterations_per_epoch):\n",
        "    \"\"\"\n",
        "    Creates the learning rate, weight decay, and teacher momentum schedules\n",
        "    as pre-calculated NumPy arrays, matching the original DINO implementation.\n",
        "\n",
        "    Args:\n",
        "        args (args_class): An object containing training hyperparameters.\n",
        "        n_iterations_per_epoch (int): The number of training steps in one epoch.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - lr_schedule (np.array): The learning rate for each iteration.\n",
        "            - wd_schedule (np.array): The weight decay for each iteration.\n",
        "            - momentum_schedule (np.array): The teacher momentum for each iteration.\n",
        "    \"\"\"\n",
        "    # Total number of training iterations\n",
        "    n_iterations = args.EPOCHS * n_iterations_per_epoch\n",
        "\n",
        "    # Helper function to replicate the original DINO cosine scheduler\n",
        "    def cosine_scheduler(base_value, final_value, total_iters, warmup_iters=0, start_warmup_value=0):\n",
        "        warmup_schedule = np.array([])\n",
        "        if warmup_iters > 0:\n",
        "            warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
        "\n",
        "        iters = np.arange(total_iters - warmup_iters)\n",
        "        schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n",
        "\n",
        "        schedule = np.concatenate((warmup_schedule, schedule))\n",
        "        assert len(schedule) == total_iters\n",
        "        return schedule\n",
        "\n",
        "    # Linear scaling rule from the original paper: base_lr * batch_size / 256\n",
        "    # Our code is single-GPU, so we use BATCH_SIZE_PER_GPU. For multi-GPU, this would be scaled by world_size.\n",
        "    base_lr = args.LR * args.BATCH_SIZE_PER_GPU / 256.0\n",
        "\n",
        "    lr_schedule = cosine_scheduler(\n",
        "        base_value=base_lr,\n",
        "        final_value=args.MIN_LR,\n",
        "        total_iters=n_iterations,\n",
        "        warmup_iters=args.WARMUP_EPOCHS * n_iterations_per_epoch,\n",
        "        start_warmup_value=0, # Start warmup from 0\n",
        "    )\n",
        "\n",
        "    # No warmup for weight decay\n",
        "    wd_schedule = cosine_scheduler(\n",
        "        base_value=args.WEIGHT_DECAY,\n",
        "        final_value=args.WEIGHT_DECAY_END,\n",
        "        total_iters=n_iterations,\n",
        "    )\n",
        "\n",
        "    # No warmup for momentum\n",
        "    momentum_schedule = cosine_scheduler(\n",
        "        base_value=args.MOMENTUM_TEACHER,\n",
        "        final_value=1.0,\n",
        "        total_iters=n_iterations,\n",
        "    )\n",
        "\n",
        "    return lr_schedule, wd_schedule, momentum_schedule"
      ],
      "metadata": {
        "id": "xOXQn0LcXBTl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss, Train one Epoch, Augmentation\n"
      ],
      "metadata": {
        "id": "S2EFhlzPPxEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The loss function encourages a student network to match the output of a momentum teacher network.\n",
        "    This is a form of self-distillation without labels.\n",
        "\n",
        "    Args:\n",
        "        args (args_class): An object containing the necessary hyperparameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # The student temperature is constant\n",
        "        self.student_temp = args.STUDENT_TEMP\n",
        "        # The number of crops is the two global crops plus all local crops\n",
        "        self.n_crops = args.GLOBAL_CROPS_NUMBER + args.LOCAL_CROPS_NUMBER\n",
        "        self.center_momentum = args.CENTER_MOMENTUM\n",
        "        self.register_buffer(\"center\", torch.zeros(1, args.OUT_DIM)) # Initialize a non-trainable tensor and add to the module\n",
        "\n",
        "        # The teacher temperature is scheduled to warm up from an initial value to a final value.\n",
        "        # The DINO paper mentions a warmup from 0.04 to 0.07 over 30 epochs.\n",
        "        teacher_schedule  = torch.linspace(\n",
        "            args.WARMUP_TEACHER_TEMP,\n",
        "            args.TEACHER_TEMP,\n",
        "            args.EPOCHS,\n",
        "        )\n",
        "        self.register_buffer(\"teacher_temp_schedule\", teacher_schedule)\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Calculates the DINO loss.\n",
        "\n",
        "        The student is trained to match the teacher's output. The teacher's output is centered\n",
        "        and sharpened. The loss is computed as the cross-entropy between the teacher's and\n",
        "        student's probability distributions over different views.\n",
        "\n",
        "        Args:\n",
        "            student_output (torch.Tensor): The output of the student network for all crops.\n",
        "            teacher_output (torch.Tensor): The output of the teacher network for global crops.\n",
        "            epoch (int): The current training epoch, used for the temperature schedule.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The calculated DINO loss.\n",
        "        \"\"\"\n",
        "        # student side\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = student_out.chunk(self.n_crops)\n",
        "\n",
        "        # teacher side: center & sharpen\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach().chunk(2)\n",
        "\n",
        "        total_loss, n_loss_terms = 0.0, 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v, sv in enumerate(student_out):\n",
        "                if v == iq:\n",
        "                    continue\n",
        "                loss = torch.sum(-q * F.log_softmax(sv, dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "\n",
        "        total_loss /= n_loss_terms\n",
        "        self._update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update the center used for teacher output centering.\n",
        "        This is a momentum update using the global batch statistics.\n",
        "\n",
        "        This operation is performed in-place and without gradients.\n",
        "        \"\"\"\n",
        "        # In DDP, teacher_output is the output for the local batch on the current process.\n",
        "        # We need the mean of the outputs across all processes.\n",
        "        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)\n",
        "\n",
        "        # synchronize across all processes\n",
        "        if dist.is_initialized():\n",
        "            dist.all_reduce(batch_center, op=dist.ReduceOp.SUM)\n",
        "            batch_center /= dist.get_world_size()\n",
        "\n",
        "        # momentum update\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
      ],
      "metadata": {
        "id": "8QDNt2MrVtmq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(student, teacher, dino_loss, data_loader,\n",
        "                    optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
        "                    epoch, loss_scaler, args):\n",
        "    \"\"\"\n",
        "    Trains the DINO model for one epoch using bfloat16 mixed-precision.\n",
        "\n",
        "    Args:\n",
        "        student (nn.Module): The student network.\n",
        "        teacher (nn.Module): The teacher network.\n",
        "        dino_loss (nn.Module): The DINO loss function.\n",
        "        data_loader (torch.utils.data.DataLoader): The data loader for the training set.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer for the student network.\n",
        "        lr_schedule (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler.\n",
        "        wd_schedule (torch.optim.lr_scheduler._LRScheduler): The weight decay scheduler.\n",
        "        momentum_schedule (torch.optim.lr_scheduler._LRScheduler): The momentum scheduler for the teacher.\n",
        "        epoch (int): The current epoch number.\n",
        "        loss_scaler (torch.cuda.amp.GradScaler): The gradient scaler for mixed-precision training.\n",
        "        args (args_class): The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss for the epoch.\n",
        "    \"\"\"\n",
        "    pbar = tqdm(data_loader)\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for it, (images, _) in enumerate(pbar):\n",
        "        # Update learning rate and weight decay per iteration\n",
        "        for i, param_group in enumerate(optimizer.param_groups):\n",
        "            param_group[\"lr\"] = lr_schedule[it]\n",
        "            if i == 0:  # only the first group is regularized\n",
        "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
        "\n",
        "        # Move images to the GPU\n",
        "        images = [im.cuda(non_blocking=True) for im in images]\n",
        "\n",
        "        # Forward pass with bfloat16 automatic mixed-precision\n",
        "        # Note: bfloat16 requires NVIDIA Ampere (e.g. A100) or newer GPUs.\n",
        "        with torch.cuda.amp.autocast(enabled=args.USE_FP16, dtype=torch.bfloat16):\n",
        "            teacher_output = teacher(images[:2]) # Global crops only\n",
        "            student_output = student(images)\n",
        "            loss = dino_loss(student_output, teacher_output, epoch)\n",
        "\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"Loss is {loss}, stopping training\")\n",
        "            # Save model weights before exiting\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'student_state_dict': student.state_dict(),\n",
        "                'teacher_state_dict': teacher.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "            }, 'model_checkpoint_infinite_loss.pth')\n",
        "            print(\"Model weights saved to model_checkpoint_infinite_loss.pth\")\n",
        "            import sys\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Backward pass and student update\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # The GradScaler scales the loss to prevent underflow\n",
        "        loss_scaler.scale(loss).backward()\n",
        "        if args.CLIP_GRAD > 0:\n",
        "            # Unscale gradients before clipping\n",
        "            loss_scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(student.parameters(), args.CLIP_GRAD)\n",
        "        loss_scaler.step(optimizer)\n",
        "        loss_scaler.update()\n",
        "\n",
        "        # EMA update for the teacher network\n",
        "        with torch.no_grad():\n",
        "            m = momentum_schedule[it]\n",
        "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
        "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
        "\n",
        "        # Logging\n",
        "        torch.cuda.synchronize()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_description(f\"Epoch {epoch} | Loss: {loss.item():.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "    return train_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "nGIjTGSiVtqC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentationDINO(object):\n",
        "  # define crops\n",
        "  # make it callable"
      ],
      "metadata": {
        "id": "q0xBB6arP_6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MultiCropWrapper"
      ],
      "metadata": {
        "id": "s4kMu9WbBo-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCropWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    This is essential in DINO-style self-supervised learning, where each image\n",
        "    is transformed into several \"views\" (global and local crops) and all\n",
        "    views must be processed efficiently.\n",
        "\n",
        "    Args:\n",
        "        backbone (nn.Module):  Feature extractor ( ResNet --> from pietro).\n",
        "        head (nn.Module):      head (MLP) mapping extracted features to the output space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone: nn.Module = None, head:nn.Module=None):# : hint for the expected type\n",
        "        super().__init__()\n",
        "        # Store the backbone and head modules\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, crops: List[Tensor]) -> List[Tensor]: #list of image crops (each crop is a tensor)\n",
        "        \"\"\"\n",
        "        Performs a single forward pass for multiple crops of each image.\n",
        "\n",
        "        -- Concatenate all crop tensors along the batch dimension.\n",
        "          num of crops per image multiple the batch size to get the total batch size.\n",
        "\n",
        "        -- Run the entire batch through the Mpdel to extract features.\n",
        "\n",
        "        -- Flatten spatial dimensions if needed, resulting in (N*B, D_flat).\n",
        "\n",
        "        -- Split the heading back into a list of N tensors, each of shape (B, out_dim).\n",
        "\n",
        "        Args:\n",
        "            crops (List[Tensor]):\n",
        "                A list of image batches.\n",
        "                The list length equals the number of crops per image.\n",
        "\n",
        "        Returns:\n",
        "            List[Tensor]:\n",
        "                A list of head outputs (headings), each of shape (B, out_dim),\n",
        "                in the same order as the input crops.\n",
        "        \"\"\"\n",
        "        # Concatenate all crops into one large batch\n",
        "        all_crops = torch.cat(crops, dim=0)\n",
        "\n",
        "        # Feature extraction through thr model\n",
        "        features = self.backbone(all_crops)\n",
        "\n",
        "        # Flatten--->if needed\n",
        "        if features.dim() > 2:\n",
        "            features = torch.flatten(features, start_dim=1)\n",
        "\n",
        "        # Project features to the dino head which is MLP\n",
        "        heading = self.head(features)\n",
        "\n",
        "\n",
        "        #  Split heading_inp back again\n",
        "        #  divides the first dim into `len(crops)` equal parts of size B\n",
        "        outputs = list(heading.chunk(len(crops), dim=0))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DLNYJI_B130"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RZx6z6GePtUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dino(args):\n",
        "    print(\"I am a placeholder for the main training function.\")\n",
        "    # 1. init rand seed\n",
        "    # 2. prep data (build dataset, dataloader)\n",
        "    #    data_loader = ... (Your data loading code)\n",
        "    # 3. build student, teacher, loss, optimizer\n",
        "    #    student = ...\n",
        "    #    teacher = ...\n",
        "    #    dino_loss = DINOLoss(args)\n",
        "    #    optimizer = ...\n",
        "\n",
        "    # 4. init schedulers\n",
        "    #    This is where you would call the new function\n",
        "    #    n_iter_per_epoch = len(data_loader)\n",
        "    #    lr_schedule, wd_schedule, momentum_schedule = get_dino_schedules(args, n_iter_per_epoch)\n",
        "\n",
        "    # 5. init loss_scaler\n",
        "    #    loss_scaler = torch.cuda.amp.GradScaler(enabled=args.USE_FP16)\n",
        "\n",
        "    # 6. train loop\n",
        "    # for epoch in range(args.EPOCHS):\n",
        "    #    train_one_epoch(student, teacher, dino_loss, data_loader,\n",
        "    #                    optimizer, lr_schedule, wd_schedule, momentum_schedule,\n",
        "    #                    epoch, loss_scaler, args)\n",
        "    #    # write logs, save checkpoints, etc."
      ],
      "metadata": {
        "id": "0aTeZt-DQAVo"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}