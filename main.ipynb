{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "thlaNgLVPjxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "99-LnExbP-Qc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments"
      ],
      "metadata": {
        "id": "PGU1nFicPn36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/facebookresearch/dino/blob/main/main_dino.py get_args_parser() for explanations\n",
        "\n",
        "class args_class:\n",
        "  def __init__(self):\n",
        "    # Model Parameters\n",
        "    self.MOMENTUM_TEACHER = 0.996     #default 0.996\n",
        "    self.OUT_DIM = 65536 #default 65536\n",
        "    self.NORM_LAST_LAYER = True\n",
        "    self.USE_BN_IN_HEAD = False     #default False\n",
        "\n",
        "\n",
        "    # Temperature Teacher Parameters\n",
        "    self.WARMUP_TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.WARMUP_TEACHER_TEMP_EPOCHS = 30 #default 30, but erroneously default 0 in the DINO paper?\n",
        "\n",
        "    # Training / Optimizations Parameters\n",
        "    self.USE_FP16 = True #default True\n",
        "    self.WEIGHT_DECAY = 0.04 #default 0.04\n",
        "    self.WEIGHT_DECAY_END = 0.4 # default 0.4\n",
        "    self.CLIP_GRAD = 3.0 #default 3.0\n",
        "    self.BATCH_SIZE_PER_GPU = 64 #default 64\n",
        "    self.EPOCHS = 100 #default 100\n",
        "    self.FREEZE_LAST_LAYER = 1 #default 1\n",
        "    self.LR = 0.0005 #default 0.0005\n",
        "    self.WARMUP_EPOCHS = 10 #default 10\n",
        "    self.MIN_LR = 1e-6 #default 1e-6\n",
        "    self.optimizer = 'adamw' # default adamw    TODO:Could be constructed here? not sure.\n",
        "    self.DROP_PATH_RATE = 0.1 #default 0.1\n",
        "\n",
        "    # Multi-Crop Parameters\n",
        "    self.GLOBAL_CROPS_SCALE = (0.4, 0.1) # default (0.4, 0.1)\n",
        "    self.LOCAL_CROPS_NUMBER = 8 #default 8\n",
        "    self.LOCAL_CROPS_SCALE = (0.05, 0.4) #default (0.05, 0.4)\n",
        "\n",
        "    #Misc\n",
        "    self.num_works = 10 # default 10\n",
        "    # TODO: imgnet directory, how to store weights?\n",
        "\n",
        "args = args_class()"
      ],
      "metadata": {
        "id": "h0jKeWbKP_kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture, DINOHead"
      ],
      "metadata": {
        "id": "VByRjDOMQejt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBZl_GfjQgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "PcA7dQqLXi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOXQn0LcXBTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss, Train one Epoch, Augmentation\n"
      ],
      "metadata": {
        "id": "S2EFhlzPPxEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOLoss(nn.module):\n",
        "  # init vars\n",
        "  # forward\n",
        "    # scale student\n",
        "    # center teacher\n",
        "    # cross entropy\n",
        "    # update center\n",
        "    # return loss"
      ],
      "metadata": {
        "id": "8QDNt2MrVtmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  # for it\n",
        "    # update weights + lr\n",
        "    # imgs to GPU\n",
        "    # Forward Pass + loss\n",
        "\n",
        "    # Update Student\n",
        "\n",
        "    # EMA teacher\n",
        "\n",
        "    # logging"
      ],
      "metadata": {
        "id": "nGIjTGSiVtqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentationDINO(object):\n",
        "  # define crops\n",
        "  # make it callable"
      ],
      "metadata": {
        "id": "q0xBB6arP_6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MultiCropWrapper"
      ],
      "metadata": {
        "id": "s4kMu9WbBo-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCropWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    This is essential in DINO-style self-supervised learning, where each image\n",
        "    is transformed into several \"views\" (global and local crops) and all\n",
        "    views must be processed efficiently.\n",
        "\n",
        "    Args:\n",
        "        backbone (nn.Module):  Feature extractor ( ResNet --> from pietro).\n",
        "        head (nn.Module):      head (MLP) mapping extracted features to the output space.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone: nn.Module = None, head:nn.Module=None):# : hint for the expected type\n",
        "        super().__init__()\n",
        "        # Store the backbone and head modules\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, crops: List[Tensor]) -> List[Tensor]: #list of image crops (each crop is a tensor)\n",
        "        \"\"\"\n",
        "        Performs a single forward pass for multiple crops of each image.\n",
        "\n",
        "        -- Concatenate all crop tensors along the batch dimension.\n",
        "          num of crops per image multiple the batch size to get the total batch size.\n",
        "\n",
        "        -- Run the entire batch through the Mpdel to extract features.\n",
        "\n",
        "        -- Flatten spatial dimensions if needed, resulting in (N*B, D_flat).\n",
        "\n",
        "        -- Split the heading back into a list of N tensors, each of shape (B, out_dim).\n",
        "\n",
        "        Args:\n",
        "            crops (List[Tensor]):\n",
        "                A list of image batches.\n",
        "                The list length equals the number of crops per image.\n",
        "\n",
        "        Returns:\n",
        "            List[Tensor]:\n",
        "                A list of head outputs (headings), each of shape (B, out_dim),\n",
        "                in the same order as the input crops.\n",
        "        \"\"\"\n",
        "        # Concatenate all crops into one large batch\n",
        "        all_crops = torch.cat(crops, dim=0)\n",
        "\n",
        "        # Feature extraction through thr model\n",
        "        features = self.backbone(all_crops)\n",
        "\n",
        "        # Flatten--->if needed\n",
        "        if features.dim() > 2:\n",
        "            features = torch.flatten(features, start_dim=1)\n",
        "\n",
        "        # Project features to the dino head which is MLP\n",
        "        heading = self.head(features)\n",
        "\n",
        "\n",
        "        #  Split heading_inp back again\n",
        "        #  divides the first dim into `len(crops)` equal parts of size B\n",
        "        outputs = list(heading.chunk(len(crops), dim=0))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7DLNYJI_B130"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RZx6z6GePtUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dino(args):\n",
        "  # init rand seed\n",
        "  # prep data\n",
        "  # build student, teacher ???\n",
        "  # add multicrop wrapper\n",
        "  # init loss\n",
        "  # init optimizer\n",
        "  # init schedulers\n",
        "  # train\n",
        "    # train_one_epoch\n",
        "    # write logs"
      ],
      "metadata": {
        "id": "0aTeZt-DQAVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}