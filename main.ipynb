{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "thlaNgLVPjxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "99-LnExbP-Qc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments"
      ],
      "metadata": {
        "id": "PGU1nFicPn36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/facebookresearch/dino/blob/main/main_dino.py get_args_parser() for explanations\n",
        "\n",
        "class args_class:\n",
        "  def __init__(self):\n",
        "    # Model Parameters\n",
        "    self.MOMENTUM_TEACHER = 0.996     #default 0.996\n",
        "    self.OUT_DIM = 65536 #default 65536\n",
        "    self.NORM_LAST_LAYER = True\n",
        "    self.USE_BN_IN_HEAD = False     #default False\n",
        "\n",
        "\n",
        "    # Temperature Teacher Parameters\n",
        "    self.WARMUP_TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.WARMUP_TEACHER_TEMP_EPOCHS = 30 #default 30, but erroneously default 0 in the DINO paper?\n",
        "\n",
        "    # Training / Optimizations Parameters\n",
        "    self.USE_FP16 = True #default True\n",
        "    self.WEIGHT_DECAY = 0.04 #default 0.04\n",
        "    self.WEIGHT_DECAY_END = 0.4 # default 0.4\n",
        "    self.CLIP_GRAD = 3.0 #default 3.0\n",
        "    self.BATCH_SIZE_PER_GPU = 64 #default 64\n",
        "    self.EPOCHS = 100 #default 100\n",
        "    self.FREEZE_LAST_LAYER = 1 #default 1\n",
        "    self.LR = 0.0005 #default 0.0005\n",
        "    self.WARMUP_EPOCHS = 10 #default 10\n",
        "    self.MIN_LR = 1e-6 #default 1e-6\n",
        "    self.optimizer = 'adamw' # default adamw    TODO:Could be constructed here? not sure.\n",
        "    self.DROP_PATH_RATE = 0.1 #default 0.1\n",
        "\n",
        "    # Multi-Crop Parameters\n",
        "    self.GLOBAL_CROPS_SCALE = (0.4, 0.1) # default (0.4, 0.1)\n",
        "    self.LOCAL_CROPS_NUMBER = 8 #default 8\n",
        "    self.LOCAL_CROPS_SCALE = (0.05, 0.4) #default (0.05, 0.4)\n",
        "\n",
        "    #Misc\n",
        "    self.num_works = 10 # default 10\n",
        "    # TODO: imgnet directory, how to store weights?\n",
        "\n",
        "args = args_class()"
      ],
      "metadata": {
        "id": "h0jKeWbKP_kT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture, DINOHead"
      ],
      "metadata": {
        "id": "VByRjDOMQejt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBZl_GfjQgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "PcA7dQqLXi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOXQn0LcXBTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss, Train one Epoch, Augmentation\n"
      ],
      "metadata": {
        "id": "S2EFhlzPPxEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The loss function encourages a student network to match the output of a momentum teacher network.\n",
        "    This is a form of self-distillation without labels.\n",
        "\n",
        "    Args:\n",
        "        args (args_class): An object containing the necessary hyperparameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        # The student temperature is constant\n",
        "        self.student_temp = 0.1\n",
        "        # The number of crops is the two global crops plus all local crops\n",
        "        self.n_crops = 2 + args.LOCAL_CROPS_NUMBER\n",
        "        self.center_momentum = 0.9\n",
        "        self.register_buffer(\"center\", torch.zeros(1, args.OUT_DIM)) # Initialize a non-trainable tensor and add to the module\n",
        "\n",
        "        # The teacher temperature is scheduled to warm up from an initial value to a final value.\n",
        "        # The DINO paper mentions a warmup from 0.04 to 0.07 over 30 epochs.\n",
        "        self.teacher_temp_schedule = torch.linspace(\n",
        "            args.WARMUP_TEACHER_TEMP,\n",
        "            args.TEACHER_TEMP,\n",
        "            args.EPOCHS,\n",
        "        )\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Calculates the DINO loss.\n",
        "\n",
        "        The student is trained to match the teacher's output. The teacher's output is centered\n",
        "        and sharpened. The loss is computed as the cross-entropy between the teacher's and\n",
        "        student's probability distributions over different views.\n",
        "\n",
        "        Args:\n",
        "            student_output (torch.Tensor): The output of the student network for all crops.\n",
        "            teacher_output (torch.Tensor): The output of the teacher network for global crops.\n",
        "            epoch (int): The current training epoch, used for the temperature schedule.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The calculated DINO loss.\n",
        "        \"\"\"\n",
        "        # Scale student output by the student temperature\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = student_out.chunk(self.n_crops)\n",
        "\n",
        "        # Center and sharpen teacher output\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach().chunk(2) # Teacher only processes the 2 global views\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                if v == iq:\n",
        "                    # Skip comparing a global view to itself\n",
        "                    continue\n",
        "                # Calculate cross-entropy between a teacher view and a student view\n",
        "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Updates the center for the teacher's output using an exponential moving average.\n",
        "        This operation helps to prevent model collapse.\n",
        "\n",
        "        Args:\n",
        "            teacher_output (torch.Tensor): The output of the teacher network.\n",
        "        \"\"\"\n",
        "        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)"
      ],
      "metadata": {
        "id": "8QDNt2MrVtmq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  # for it\n",
        "    # update weights + lr\n",
        "    # imgs to GPU\n",
        "    # Forward Pass + loss\n",
        "\n",
        "    # Update Student\n",
        "\n",
        "    # EMA teacher\n",
        "\n",
        "    # logging"
      ],
      "metadata": {
        "id": "nGIjTGSiVtqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentationDINO(object):\n",
        "  # define crops\n",
        "  # make it callable"
      ],
      "metadata": {
        "id": "q0xBB6arP_6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RZx6z6GePtUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dino(args):\n",
        "  # init rand seed\n",
        "  # prep data\n",
        "  # build student, teacher ???\n",
        "  # add multicrop wrapper\n",
        "  # init loss\n",
        "  # init optimizer\n",
        "  # init schedulers\n",
        "  # train\n",
        "    # train_one_epoch\n",
        "    # write logs"
      ],
      "metadata": {
        "id": "0aTeZt-DQAVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}