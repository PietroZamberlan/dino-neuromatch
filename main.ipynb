{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON4vE8PUT6kPXikSn04cz3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "thlaNgLVPjxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import datetime\n",
        "import subprocess\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from PIL import ImageFilter, ImageOps"
      ],
      "metadata": {
        "id": "99-LnExbP-Qc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "PcA7dQqLXi7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
      ],
      "metadata": {
        "id": "xOXQn0LcXBTl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arguments"
      ],
      "metadata": {
        "id": "PGU1nFicPn36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://github.com/facebookresearch/dino/blob/main/main_dino.py get_args_parser() for explanations\n",
        "\n",
        "class args_class:\n",
        "  def __init__(self):\n",
        "    # Model Parameters\n",
        "    self.MOMENTUM_TEACHER = 0.996     #default 0.996\n",
        "    self.OUT_DIM = 65536 #default 65536\n",
        "    self.NORM_LAST_LAYER = True\n",
        "    self.USE_BN_IN_HEAD = False     #default False\n",
        "\n",
        "\n",
        "    # Temperature Teacher Parameters\n",
        "    self.WARMUP_TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.TEACHER_TEMP = 0.04   #default 0.04\n",
        "    self.WARMUP_TEACHER_TEMP_EPOCHS = 30 #default 30, but erroneously default 0 in the DINO paper?\n",
        "\n",
        "    # Training / Optimizations Parameters\n",
        "    self.USE_FP16 = True #default True\n",
        "    self.WEIGHT_DECAY = 0.04 #default 0.04\n",
        "    self.WEIGHT_DECAY_END = 0.4 # default 0.4\n",
        "    self.CLIP_GRAD = 3.0 #default 3.0\n",
        "    self.BATCH_SIZE_PER_GPU = 64 #default 64\n",
        "    self.EPOCHS = 100 #default 100\n",
        "    self.FREEZE_LAST_LAYER = 1 #default 1\n",
        "    self.LR = 0.0005 #default 0.0005\n",
        "    self.WARMUP_EPOCHS = 10 #default 10\n",
        "    self.MIN_LR = 1e-6 #default 1e-6\n",
        "    self.optimizer = 'adamw' # default adamw    TODO:Could be constructed here? not sure.\n",
        "    self.DROP_PATH_RATE = 0.1 #default 0.1\n",
        "\n",
        "    # Multi-Crop Parameters\n",
        "    self.GLOBAL_CROPS_SCALE = (0.4, 0.1) # default (0.4, 0.1)\n",
        "    self.LOCAL_CROPS_NUMBER = 8 #default 8\n",
        "    self.LOCAL_CROPS_SCALE = (0.05, 0.4) #default (0.05, 0.4)\n",
        "\n",
        "    #Misc\n",
        "    self.num_works = 10 # default 10\n",
        "    # TODO: imgnet directory, how to store weights?\n",
        "\n",
        "args = args_class()"
      ],
      "metadata": {
        "id": "h0jKeWbKP_kT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture, DINOHead"
      ],
      "metadata": {
        "id": "VByRjDOMQejt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBZl_GfjQgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOHead(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
        "    super().__init__()\n",
        "    nlayers = max(nlayers, 1)\n",
        "    if nlayers == 1:\n",
        "      self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
        "    else:\n",
        "      layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "      if use_bn:\n",
        "        layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "      layers.append(nn.GELU())\n",
        "      for _ in range(nlayers - 2):\n",
        "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        if use_bn:\n",
        "          layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "      layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "      self.mlp = nn.Sequential(*layers)\n",
        "    self.apply(self._init_weights)\n",
        "    self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
        "    self.last_layer.weight_g.data.fill_(1)\n",
        "    if norm_last_layer:\n",
        "      self.last_layer.weight_g.requires_grad = False\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "        trunc_normal_(m.weight, std=.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "          nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.mlp(x)\n",
        "      x = nn.functional.normalize(x, dim=-1, p=2)\n",
        "      x = self.last_layer(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "azsDsBe-zl_H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss, Train one Epoch, Augmentation\n"
      ],
      "metadata": {
        "id": "S2EFhlzPPxEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOLoss(nn.module):\n",
        "  # init vars\n",
        "  # forward\n",
        "    # scale student\n",
        "    # center teacher\n",
        "    # cross entropy\n",
        "    # update center\n",
        "    # return loss"
      ],
      "metadata": {
        "id": "8QDNt2MrVtmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  # for it\n",
        "    # update weights + lr\n",
        "    # imgs to GPU\n",
        "    # Forward Pass + loss\n",
        "\n",
        "    # Update Student\n",
        "\n",
        "    # EMA teacher\n",
        "\n",
        "    # logging"
      ],
      "metadata": {
        "id": "nGIjTGSiVtqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentationDINO(object):\n",
        "  # define crops\n",
        "  # make it callable"
      ],
      "metadata": {
        "id": "q0xBB6arP_6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "RZx6z6GePtUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dino(args):\n",
        "  # init rand seed\n",
        "  # prep data\n",
        "  # build student, teacher ???\n",
        "  # add multicrop wrapper\n",
        "  # init loss\n",
        "  # init optimizer\n",
        "  # init schedulers\n",
        "  # train\n",
        "    # train_one_epoch\n",
        "    # write logs"
      ],
      "metadata": {
        "id": "0aTeZt-DQAVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}